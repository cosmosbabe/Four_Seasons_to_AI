{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a1cfa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efb3fed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import option_context\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8ef05ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import utils\n",
    "import torch\n",
    "\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "\n",
    "import seaborn as sns\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8856bc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import probe\n",
    "\n",
    "# there are some folders of the GitHub code built as libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba261899",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20.5px; text-align: center\"> Language Models Represent Space and Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b99379",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 15px; text-align: center\"> Wes Gurnee and Max Tegmark, Massachusetts Institute of Technology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f423e24f",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20; text-align: center; color: gray\"> Resume by Irina Nedyalkova, Deep Learning Student"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f47ad2",
   "metadata": {},
   "source": [
    "Max Tegmark and Wes Gurnee discover that $Large$ $Language$ $Models$ (LLMs) learn linear representations of space and time across multiple scales. The two analyze learned representations of three spatial datasets - World, US and NYC places, and three temporal datasets - historical figures, artworks and news headlines, in the Llama-2 family of models. In addition, they indentify individual \"space neurons\" and \"time neurons\" that reliably encode spatial and temporal coordinates. The result of their paper suggests that modern LLMs learn rich spatio-temporal representations of the real world and possess basic ingredients of a world model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466ef272",
   "metadata": {},
   "source": [
    "Despite being trained to just predict the next token, Large Language Models ($LLMs$) have demonstrated an impressive set of capabilities, raising questions about what such models have actually learned. One hypothesis is that LLMs learn a massive collection of correlations but lack any understanding of the data. An alternative hypothesis is that LLMs, in the course of compressing the data, learn more compact, coherent and interpretable models of the generative process underlying the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e869372",
   "metadata": {},
   "source": [
    "In this work, Gurnee and Tegmark, raise the question of whether LLMs form World (and temporal) Models as literally as possible - they attempt to extract an actual map of the world. While such spatio-temporal representations do not constitute a dynamic causal World Model in their own right, having coherent multi-scale representations of space and time are basic ingredients required in a more comprehensive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f988c94",
   "metadata": {},
   "source": [
    "There are six datasets containing the names of places or events with corresponding space or time coordinates that span multiple spatio-temporal scales:\n",
    "- locations within the whole world in addition to the death year of historical figures from the past 3000 years;\n",
    "- the release date of art and entertainment from 1950s onward;\n",
    "- the publication date of news headlines from 2010 to 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d00d77",
   "metadata": {},
   "source": [
    "Using Llama-2 family of models they train linear regression probes on the internal activations of the names of these places and events at each layer to predict the real-world location (latitude/longitude) or time (numeric timestamp). These probing experiments reveal evidence that models build spatial and temporal representations throughout the early layers before plateauing at around the model halfway point with larger models consistently outperforming smaller ones. These representations are linear, given that nonlinear probes do not perform better, fairly robust to changes in prompting and unified across different kinds of entities (cities or landmarks). Finally, the probes are used to find individual neurons which activate as a function of space or time, providing strong evidence that the model is truly using these features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f8e72d",
   "metadata": {},
   "source": [
    "Here are entity counts and representative examples for each dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d398008d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Count</th>\n",
       "      <th>Examples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>World</td>\n",
       "      <td>39585</td>\n",
       "      <td>\"Los Angeles\", \"St.Peter's Basilica\", \"Canary Islands\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USA</td>\n",
       "      <td>29997</td>\n",
       "      <td>\"Fenway Park\", \"Columbia University\", \"Riverside County\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NYC</td>\n",
       "      <td>19838</td>\n",
       "      <td>\"Borden Avenue Bridge\", \"Trump International Hotel\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Figures</td>\n",
       "      <td>37539</td>\n",
       "      <td>\"Cleopatra\", \"Dante Aleghieri\", \"Carl Sagan\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Artworks</td>\n",
       "      <td>31321</td>\n",
       "      <td>\"Stephen King's It\", \"Queens Bohemian Rhapsody\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Headlines</td>\n",
       "      <td>28389</td>\n",
       "      <td>\"Pilgrims, Fewer and Socially Distanced, Arrived in Mecca for Annual Hajj\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Dataset  Count  \\\n",
       "0      World  39585   \n",
       "1        USA  29997   \n",
       "2        NYC  19838   \n",
       "3    Figures  37539   \n",
       "4   Artworks  31321   \n",
       "5  Headlines  28389   \n",
       "\n",
       "                                                                     Examples  \n",
       "0                      \"Los Angeles\", \"St.Peter's Basilica\", \"Canary Islands\"  \n",
       "1                    \"Fenway Park\", \"Columbia University\", \"Riverside County\"  \n",
       "2                         \"Borden Avenue Bridge\", \"Trump International Hotel\"  \n",
       "3                                \"Cleopatra\", \"Dante Aleghieri\", \"Carl Sagan\"  \n",
       "4                             \"Stephen King's It\", \"Queens Bohemian Rhapsody\"  \n",
       "5  \"Pilgrims, Fewer and Socially Distanced, Arrived in Mecca for Annual Hajj\"  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)      # shows whole sentence in the column\n",
    "\n",
    "pd.read_excel(\"a1 datasets example.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf589bc2",
   "metadata": {},
   "source": [
    "All the experiments are run with the base Llama-2 series of auto-regressive transformer language models, spanning 7 billion to 70 billion parameters. For each dataset is ran every entity name through the model, potentially prepended with a short prompt and saved the activations of the hidden state (residual stream - the aggregation of the outputs from prior layers) on the last entity token for each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716f920b",
   "metadata": {},
   "source": [
    "To find evidence of spatial and temporal representations in LLMs, Gurnee and Tegmark use standard technique of probing, which fits a simple model on the network activations to predict some target label associated with labeled input data - given an activation dataset and a target (containing either the time or two-dimensional latitude and longitude coordinates) fit linear ridge regression probes yielding a linear predictor. High predictive performance on out-of-sample data indicates that the base model has temporal and spatial information linearly decodable in its representations (although this does not imply that the model actually uses these representations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15eb1186",
   "metadata": {},
   "source": [
    "And to evaluate the performance of the probes they report standard regression metrics such as $R^2$ and Spearman rank correlation on the test data (correlations averaged over latitude and longitude for spatial features). An additional metric they compute is the proximity error for each prediction, defined as the fraction of entities predicted to be closer to the target point than the prediction of the target entity. The intuition is that for spatial data, absolute error metrics can be misleading - a 500 km error for a city on the East Coast of the United States is far more significant than a 500 km error in\n",
    "Siberia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "24b573eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "world_places = pd.read_csv(\"world_places.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0141b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_places = pd.read_csv(\"us_places.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "755dd015",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_places = pd.read_csv(\"nyc_places.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cf1cadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "figures = pd.read_csv(\"historical_figures.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d71e942a",
   "metadata": {},
   "outputs": [],
   "source": [
    "artworks = pd.read_csv(\"artworks.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6966381c",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = pd.read_csv(\"headlines.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb0aa603",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some source code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442e36b5",
   "metadata": {},
   "source": [
    "Do models represent space and time at all? If so, where internally in the model? Does the representation quality change substantially with model scale?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227576a7",
   "metadata": {},
   "source": [
    "Both, spatial and temporal features, can be recovered with a linear probe. These representations smoothly increase in quality throughout the first half of the layers of the model before reaching a plateau and the representations are more accurate with increasing model scale. The dataset with the worst performance is the New York City dataset but this is also the dataset where the largest model has the best relative performance, suggesting that sufficiently large LLMs could eventually form detailed spatial models of individual cities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37e2d83",
   "metadata": {},
   "source": [
    "Here I display Out-of-sample $R^2$ of linear and nonlinear (one layer MLP) probes for all models and\n",
    "features at 60% layer depth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f3d95c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Probe</th>\n",
       "      <th>World</th>\n",
       "      <th>USA</th>\n",
       "      <th>NYC</th>\n",
       "      <th>Figures</th>\n",
       "      <th>Artworks</th>\n",
       "      <th>Headlines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Llama-2-7b</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.881</td>\n",
       "      <td>0.799</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.788</td>\n",
       "      <td>0.564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Llama-2-7b</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.819</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.746</td>\n",
       "      <td>0.467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Llama-2-13b</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.896</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.804</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama-2-13b</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.916</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Llama-2-70b</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.911</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.359</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Llama-2-70b</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.926</td>\n",
       "      <td>0.869</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.839</td>\n",
       "      <td>0.884</td>\n",
       "      <td>0.739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Model   Probe  World    USA    NYC  Figures  Artworks  Headlines\n",
       "0   Llama-2-7b  Linear  0.881  0.799  0.219    0.785     0.788      0.564\n",
       "1   Llama-2-7b     MLP  0.897  0.819  0.204    0.775     0.746      0.467\n",
       "2  Llama-2-13b  Linear  0.896  0.825  0.237    0.804     0.806      0.645\n",
       "3  Llama-2-13b     MLP  0.916  0.824  0.230    0.818     0.808      0.656\n",
       "4  Llama-2-70b  Linear  0.911  0.864  0.359    0.835     0.885      0.746\n",
       "5  Llama-2-70b     MLP  0.926  0.869  0.312    0.839     0.884      0.739"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_excel(\"a1 table2.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e2613e",
   "metadata": {},
   "source": [
    "To test whether spatial and temporal features are represented linearly, the two compared the performance of the linear ridge regression probes with that of substantially more expressive nonlinear MLP probes of the form $W_2ReLU(W_1x + b_1) + b_2$ with 256 neurons. The table above reports the results and shows that using nonlinear probes results in minimal improvement to $R^2$ for any dataset or model. We all take this as strong evidence that space and time are also represented linearly (or at the very least are linearly decodable), despite being continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ebdaf9",
   "metadata": {},
   "source": [
    "to be continued..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5615333a",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38304b87",
   "metadata": {},
   "source": [
    "Original paper: https://arxiv.org/pdf/2310.02207v2.pdf\n",
    "Source code: https://github.com/wesg52/world-models/tree/main"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
